{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6391b75c-2c3f-4201-8b49-9a633f4e6d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocess as mp\n",
    "import jieba.posseg as pseg\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15846fda-0d45-4368-8169-0a2531b3328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_PATH = 'train.csv'\n",
    "TEST_CSV_PATH = 'test.csv'\n",
    "TOKENIZED_TRAIN_CSV_PATH = 'tokenized_train.csv'\n",
    "TOKENIZED_TEST_CSV_PATH = 'tokenized_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5285a51-83db-448d-af09-445c498be9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>There are two new old-age insurance benefits f...</td>\n",
       "      <td>Police disprove \"bird's nest congress each per...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tid1  tid2                          title1_zh                  title2_zh  \\\n",
       "id                                                                             \n",
       "0      0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "3      2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "1      2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港       GDP首超香港？深圳澄清：还差一点点……   \n",
       "\n",
       "                                            title1_en  \\\n",
       "id                                                      \n",
       "0   There are two new old-age insurance benefits f...   \n",
       "3   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "1   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "\n",
       "                                            title2_en      label  \n",
       "id                                                                \n",
       "0   Police disprove \"bird's nest congress each per...  unrelated  \n",
       "3   Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n",
       "1   The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_CSV_PATH, index_col='id')\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37752817-5b72-41d1-8428-e8d83cb0c126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title1_zh                  title2_zh      label\n",
       "id                                                                         \n",
       "0       2017养老保险又新增两项，农村老人人人可申领，你领到了吗   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京  unrelated\n",
       "3   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小  unrelated\n",
       "1   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港       GDP首超香港？深圳澄清：还差一点点……  unrelated"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.loc[:, ['title1_zh', 'title2_zh', 'label']]\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f94771-8822-43bb-8117-15de0aa15031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title1_zh    False\n",
       "title2_zh     True\n",
       "label        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c1ead84-e9e6-48f2-b2c2-f7238e97d769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title1_zh    False\n",
       "title2_zh    False\n",
       "label        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.title2_zh.fillna('UNKNOWN', inplace=True)\n",
    "train.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b16ced-0463-4961-a108-24867489f0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:10: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "/var/folders/dx/2th7_vf107dbw1xc0g6h69180000gn/T/ipykernel_10927/3440435041.py:10: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert((data.index == res.index).all(), 'Something error when merge data')\n"
     ]
    }
   ],
   "source": [
    "def jieba_tokenizer(text):\n",
    "    words = pseg.cut(text)\n",
    "    return ' '.join([word for word, flag in words if flag != 'x'])\n",
    "\n",
    "def process(data):\n",
    "    res = data.apply(jieba_tokenizer)\n",
    "    return res\n",
    "\n",
    "def check_merge_idx(data, res):\n",
    "    assert((data.index == res.index).all(), 'Something error when merge data')\n",
    "\n",
    "def parallelize(data, func):\n",
    "    cores = partitions = mp.cpu_count()\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    pool = mp.Pool(cores)\n",
    "    res = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    check_merge_idx(data, res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9ecbee1-b91c-44c3-a882-ac431cde03c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(train.index == train.title1_zh.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2faea7cf-8fa9-4efb-b439-22504c919bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use prepared tokenized train data\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TOKENIZED_TRAIN_CSV_PATH):\n",
    "    print('Use prepared tokenized train data')\n",
    "    train = pd.read_csv(TOKENIZED_TRAIN_CSV_PATH, index_col='id')\n",
    "else:\n",
    "    print('Start to training')\n",
    "    train['title1_tokenized'] = parallelize(train.loc[:, 'title1_zh'], process)\n",
    "    train['title2_tokenized'] = parallelize(train.loc[:, 'title2_zh'], process)\n",
    "    train.to_csv('tokenized_train.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3525ca53-55ac-47d9-95df-ec4a93213696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>label</th>\n",
       "      <th>title1_tokenized</th>\n",
       "      <th>title2_tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗</td>\n",
       "      <td>警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "      <td>深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "      <td>GDP 首 超 香港 深圳 澄清 还 差 一点点</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title1_zh                  title2_zh      label  \\\n",
       "id                                                                            \n",
       "0       2017养老保险又新增两项，农村老人人人可申领，你领到了吗   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京  unrelated   \n",
       "3   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小  unrelated   \n",
       "1   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港       GDP首超香港？深圳澄清：还差一点点……  unrelated   \n",
       "\n",
       "                                   title1_tokenized  \\\n",
       "id                                                    \n",
       "0          2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗   \n",
       "3   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港   \n",
       "1   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港   \n",
       "\n",
       "                      title2_tokenized  \n",
       "id                                      \n",
       "0    警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京  \n",
       "3   深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小  \n",
       "1             GDP 首 超 香港 深圳 澄清 还 差 一点点  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.fillna('UNKNOWN', inplace=True)\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c52a88-b226-4941-be31-89b05da1a6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(641104,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_x1 = train.title1_tokenized\n",
    "corpus_x2 = train.title2_tokenized\n",
    "corpus = pd.concat([corpus_x1, corpus_x2])\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2879f694-c9a6-42f5-a5e6-11ad96c77042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title\n",
       "id                                                 \n",
       "0          2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗\n",
       "3   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
       "1   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
       "2   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
       "9                        用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(corpus.iloc[:5], columns=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ab70b47-ed23-4ebc-ae0b-727eb53879d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdee2b44-472b-4fcb-88b0-80ff55caeba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "x1_train = tokenizer.texts_to_sequences(corpus_x1)\n",
    "x2_train = tokenizer.texts_to_sequences(corpus_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b0325dc-648c-4a29-ba7e-86484ef746b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320552"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2e81d42-8058-4703-bdce-dc261e5fee32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[217, 1268, 32, 1178, 5967, 25, 489, 2877, 116, 5559, 4, 1850, 2, 13]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a3d9198-1bf6-4ed9-8f57-a0906d443d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017', '养老保险', '又', '新增', '两项', '农村', '老人', '人人', '可', '申领', '你', '领到', '了', '吗']\n"
     ]
    }
   ],
   "source": [
    "for seq in x1_train[:1]:\n",
    "    print([tokenizer.index_word[idx] for idx in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45490b66-7f30-4d74-86f8-9c354d087bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 20\n",
    "x1_train = keras.preprocessing.sequence.pad_sequences(x1_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x2_train = keras.preprocessing.sequence.pad_sequences(x2_train, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46f8196a-36ec-4654-aca2-b2014a97e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in x1_train + x2_train:\n",
    "    assert len(seq) == 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2d7b5fd-b9a8-4f10-80b7-a9cc7f652aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0    unrelated\n",
       "3    unrelated\n",
       "1    unrelated\n",
       "2    unrelated\n",
       "9       agreed\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a837813c-a2a4-409c-8fef-aaaf3b42a39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_index = {\n",
    "    'unrelated': 0, \n",
    "    'agreed': 1, \n",
    "    'disagreed': 2\n",
    "}\n",
    "y_train = train.label.apply(lambda x: label_to_index[x])\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93406f7a-f475-4de9-b0d2-38207e1f7996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a097b89-c3f2-4820-b360-d65e2b4523d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_RATIO = 0.1\n",
    "RANDOM_STATE = 9527\n",
    "\n",
    "x1_train, x1_val, x2_train, x2_val, y_train, y_val = train_test_split(\n",
    "    x1_train, x2_train, y_train, test_size=VALIDATION_RATIO, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb199fbf-ba33-4b44-b145-e172727d44ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "x1_train: (288496, 20)\n",
      "x2_train: (288496, 20)\n",
      "y_train : (288496, 3)\n",
      "----------\n",
      "x1_val:   (32056, 20)\n",
      "x2_val:   (32056, 20)\n",
      "y_val :   (32056, 3)\n",
      "----------\n",
      "Test Set\n"
     ]
    }
   ],
   "source": [
    "print('Training Set')\n",
    "print('-' * 10)\n",
    "print(f'x1_train: {x1_train.shape}')\n",
    "print(f'x2_train: {x2_train.shape}')\n",
    "print(f'y_train : {y_train.shape}')\n",
    "print('-' * 10)\n",
    "print(f'x1_val:   {x1_val.shape}')\n",
    "print(f'x2_val:   {x2_val.shape}')\n",
    "print(f'y_val :   {y_val.shape}')\n",
    "print('-' * 10)\n",
    "print('Test Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6bd40ee-1fc7-4a6d-b0de-cd720f4f7149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 16:45:24.313273: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 256)      2560000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 128)          197120      embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            771         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 2,757,891\n",
      "Trainable params: 2,757,891\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 3\n",
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "NUM_LSTM_UNITS = 128\n",
    "\n",
    "top_input = keras.Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "bm_input = keras.Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedded = embedding_layer(top_input)\n",
    "bm_embedded = embedding_layer(bm_input)\n",
    "\n",
    "shared_lstm = keras.layers.LSTM(NUM_LSTM_UNITS)\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "merged = keras.layers.concatenate([top_output, bm_output], axis=-1)\n",
    "dense = keras.layers.Dense(units=NUM_CLASSES, activation='softmax')\n",
    "predictions = dense(merged)\n",
    "\n",
    "model = keras.Model(inputs=[top_input, bm_input], outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3424fd5c-ddbe-4a06-a118-bd0034a152a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "opt = Adam(learning_rate=lr, decay=lr/50)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ddceff7-23a6-4cc5-8c9a-e3e9cf0f393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 16:45:24.757301: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564/564 [==============================] - 274s 482ms/step - loss: 0.5565 - accuracy: 0.7472 - val_loss: 0.3919 - val_accuracy: 0.8199\n",
      "Epoch 2/50\n",
      "564/564 [==============================] - 275s 487ms/step - loss: 0.3586 - accuracy: 0.8372 - val_loss: 0.3855 - val_accuracy: 0.8241\n",
      "Epoch 3/50\n",
      "564/564 [==============================] - 292s 518ms/step - loss: 0.3160 - accuracy: 0.8592 - val_loss: 0.3738 - val_accuracy: 0.8355\n",
      "Epoch 4/50\n",
      "564/564 [==============================] - 315s 559ms/step - loss: 0.2858 - accuracy: 0.8745 - val_loss: 0.3659 - val_accuracy: 0.8407\n",
      "Epoch 5/50\n",
      "564/564 [==============================] - 322s 572ms/step - loss: 0.2597 - accuracy: 0.8869 - val_loss: 0.3708 - val_accuracy: 0.8407\n",
      "Epoch 6/50\n",
      "564/564 [==============================] - 302s 535ms/step - loss: 0.2359 - accuracy: 0.8983 - val_loss: 0.3748 - val_accuracy: 0.8441\n",
      "Epoch 7/50\n",
      "564/564 [==============================] - 261s 464ms/step - loss: 0.2147 - accuracy: 0.9091 - val_loss: 0.3984 - val_accuracy: 0.8457\n",
      "Epoch 8/50\n",
      "564/564 [==============================] - 253s 449ms/step - loss: 0.1948 - accuracy: 0.9184 - val_loss: 0.4046 - val_accuracy: 0.8462\n",
      "Epoch 9/50\n",
      "564/564 [==============================] - 215s 381ms/step - loss: 0.1769 - accuracy: 0.9257 - val_loss: 0.4285 - val_accuracy: 0.8486\n",
      "Epoch 10/50\n",
      "564/564 [==============================] - 201s 357ms/step - loss: 0.1621 - accuracy: 0.9333 - val_loss: 0.4528 - val_accuracy: 0.8493\n",
      "Epoch 11/50\n",
      "564/564 [==============================] - 193s 342ms/step - loss: 0.1486 - accuracy: 0.9388 - val_loss: 0.4622 - val_accuracy: 0.8495\n",
      "Epoch 12/50\n",
      "564/564 [==============================] - 189s 335ms/step - loss: 0.1370 - accuracy: 0.9438 - val_loss: 0.4919 - val_accuracy: 0.8477\n",
      "Epoch 13/50\n",
      "564/564 [==============================] - 186s 331ms/step - loss: 0.1269 - accuracy: 0.9487 - val_loss: 0.5082 - val_accuracy: 0.8495\n",
      "Epoch 14/50\n",
      "564/564 [==============================] - 186s 331ms/step - loss: 0.1161 - accuracy: 0.9535 - val_loss: 0.5474 - val_accuracy: 0.8505\n",
      "Epoch 15/50\n",
      "564/564 [==============================] - 207s 367ms/step - loss: 0.1091 - accuracy: 0.9564 - val_loss: 0.5580 - val_accuracy: 0.8497\n",
      "Epoch 16/50\n",
      "564/564 [==============================] - 206s 365ms/step - loss: 0.1033 - accuracy: 0.9588 - val_loss: 0.6031 - val_accuracy: 0.8492\n",
      "Epoch 17/50\n",
      "564/564 [==============================] - 205s 364ms/step - loss: 0.0975 - accuracy: 0.9616 - val_loss: 0.5933 - val_accuracy: 0.8483\n",
      "Epoch 18/50\n",
      "564/564 [==============================] - 206s 365ms/step - loss: 0.0919 - accuracy: 0.9636 - val_loss: 0.6624 - val_accuracy: 0.8502\n",
      "Epoch 19/50\n",
      "564/564 [==============================] - 210s 372ms/step - loss: 0.0873 - accuracy: 0.9650 - val_loss: 0.6267 - val_accuracy: 0.8482\n",
      "Epoch 20/50\n",
      "564/564 [==============================] - 210s 373ms/step - loss: 0.0837 - accuracy: 0.9665 - val_loss: 0.6643 - val_accuracy: 0.8488\n",
      "Epoch 21/50\n",
      "564/564 [==============================] - 210s 373ms/step - loss: 0.0807 - accuracy: 0.9677 - val_loss: 0.6875 - val_accuracy: 0.8498\n",
      "Epoch 22/50\n",
      "564/564 [==============================] - 210s 373ms/step - loss: 0.0770 - accuracy: 0.9692 - val_loss: 0.7052 - val_accuracy: 0.8491\n",
      "Epoch 23/50\n",
      "564/564 [==============================] - 209s 371ms/step - loss: 0.0747 - accuracy: 0.9703 - val_loss: 0.7410 - val_accuracy: 0.8484\n",
      "Epoch 24/50\n",
      "564/564 [==============================] - 209s 371ms/step - loss: 0.0719 - accuracy: 0.9716 - val_loss: 0.7367 - val_accuracy: 0.8486\n",
      "Epoch 25/50\n",
      "564/564 [==============================] - 212s 376ms/step - loss: 0.0704 - accuracy: 0.9717 - val_loss: 0.7579 - val_accuracy: 0.8476\n",
      "Epoch 26/50\n",
      "564/564 [==============================] - 212s 376ms/step - loss: 0.0680 - accuracy: 0.9729 - val_loss: 0.7773 - val_accuracy: 0.8493\n",
      "Epoch 27/50\n",
      "564/564 [==============================] - 213s 377ms/step - loss: 0.0648 - accuracy: 0.9738 - val_loss: 0.8000 - val_accuracy: 0.8485\n",
      "Epoch 28/50\n",
      "564/564 [==============================] - 211s 375ms/step - loss: 0.0638 - accuracy: 0.9742 - val_loss: 0.8069 - val_accuracy: 0.8480\n",
      "Epoch 29/50\n",
      "564/564 [==============================] - 211s 375ms/step - loss: 0.0619 - accuracy: 0.9748 - val_loss: 0.8058 - val_accuracy: 0.8470\n",
      "Epoch 30/50\n",
      "564/564 [==============================] - 212s 376ms/step - loss: 0.0621 - accuracy: 0.9746 - val_loss: 0.8200 - val_accuracy: 0.8476\n",
      "Epoch 31/50\n",
      "564/564 [==============================] - 204s 362ms/step - loss: 0.0601 - accuracy: 0.9753 - val_loss: 0.8751 - val_accuracy: 0.8464\n",
      "Epoch 32/50\n",
      "564/564 [==============================] - 189s 335ms/step - loss: 0.0582 - accuracy: 0.9759 - val_loss: 0.8810 - val_accuracy: 0.8468\n",
      "Epoch 33/50\n",
      "564/564 [==============================] - 189s 334ms/step - loss: 0.0564 - accuracy: 0.9765 - val_loss: 0.8889 - val_accuracy: 0.8481\n",
      "Epoch 34/50\n",
      "564/564 [==============================] - 189s 335ms/step - loss: 0.0561 - accuracy: 0.9767 - val_loss: 0.8693 - val_accuracy: 0.8461\n",
      "Epoch 35/50\n",
      "564/564 [==============================] - 190s 337ms/step - loss: 0.0549 - accuracy: 0.9771 - val_loss: 0.9015 - val_accuracy: 0.8465\n",
      "Epoch 36/50\n",
      "564/564 [==============================] - 190s 337ms/step - loss: 0.0553 - accuracy: 0.9765 - val_loss: 0.9065 - val_accuracy: 0.8451\n",
      "Epoch 37/50\n",
      "564/564 [==============================] - 194s 344ms/step - loss: 0.0533 - accuracy: 0.9779 - val_loss: 0.9307 - val_accuracy: 0.8475\n",
      "Epoch 38/50\n",
      "564/564 [==============================] - 213s 377ms/step - loss: 0.0524 - accuracy: 0.9778 - val_loss: 0.9277 - val_accuracy: 0.8446\n",
      "Epoch 39/50\n",
      "564/564 [==============================] - 215s 381ms/step - loss: 0.0525 - accuracy: 0.9776 - val_loss: 0.9454 - val_accuracy: 0.8464\n",
      "Epoch 40/50\n",
      "564/564 [==============================] - 215s 382ms/step - loss: 0.0517 - accuracy: 0.9776 - val_loss: 0.9757 - val_accuracy: 0.8477\n",
      "Epoch 41/50\n",
      "564/564 [==============================] - 215s 382ms/step - loss: 0.0512 - accuracy: 0.9784 - val_loss: 0.9975 - val_accuracy: 0.8477\n",
      "Epoch 42/50\n",
      "564/564 [==============================] - 215s 381ms/step - loss: 0.0503 - accuracy: 0.9782 - val_loss: 0.9735 - val_accuracy: 0.8462\n",
      "Epoch 43/50\n",
      "564/564 [==============================] - 214s 380ms/step - loss: 0.0498 - accuracy: 0.9787 - val_loss: 0.9731 - val_accuracy: 0.8451\n",
      "Epoch 44/50\n",
      "564/564 [==============================] - 214s 379ms/step - loss: 0.0475 - accuracy: 0.9794 - val_loss: 0.9979 - val_accuracy: 0.8447\n",
      "Epoch 45/50\n",
      "564/564 [==============================] - 217s 385ms/step - loss: 0.0482 - accuracy: 0.9791 - val_loss: 0.9776 - val_accuracy: 0.8457\n",
      "Epoch 46/50\n",
      "564/564 [==============================] - 217s 384ms/step - loss: 0.0484 - accuracy: 0.9787 - val_loss: 1.0437 - val_accuracy: 0.8458\n",
      "Epoch 47/50\n",
      "564/564 [==============================] - 213s 378ms/step - loss: 0.0472 - accuracy: 0.9793 - val_loss: 1.0316 - val_accuracy: 0.8468\n",
      "Epoch 48/50\n",
      "564/564 [==============================] - 201s 356ms/step - loss: 0.0465 - accuracy: 0.9792 - val_loss: 1.0275 - val_accuracy: 0.8458\n",
      "Epoch 49/50\n",
      "564/564 [==============================] - 198s 351ms/step - loss: 0.0450 - accuracy: 0.9802 - val_loss: 1.0314 - val_accuracy: 0.8461\n",
      "Epoch 50/50\n",
      "564/564 [==============================] - 217s 384ms/step - loss: 0.0471 - accuracy: 0.9787 - val_loss: 1.0411 - val_accuracy: 0.8448\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "history = model.fit(\n",
    "    x=[x1_train, x2_train], \n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=([x1_val, x2_val], y_val),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0774eb9a-b360-4308-b2c9-e37c26f33b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use prepared tokenized test data\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TOKENIZED_TEST_CSV_PATH):\n",
    "    print('Use prepared tokenized test data')\n",
    "    test = pd.read_csv(TOKENIZED_TEST_CSV_PATH, index_col='id')\n",
    "else:\n",
    "    print('Use raw test data')\n",
    "    test = pd.read_csv(TEST_CSV_PATH, index_col='id')\n",
    "    test = test.loc[:, ['title1_zh', 'title2_zh']]\n",
    "    test.fillna('UNKNOWN', inplace=True)\n",
    "    test['title1_tokenized'] = parallelize(test.loc[:, 'title1_zh'], process)\n",
    "    test['title2_tokenized'] = parallelize(test.loc[:, 'title2_zh'], process)\n",
    "    test.to_csv('tokenized_test.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef9ae48c-e723-492b-85c1-066112ca633a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_tokenized</th>\n",
       "      <th>title2_tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321187</th>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "      <td>萨拉 赫 人气 爆棚 埃及 总统大选 未 参选 获 百万 选票 现任 总统 压力 山 大</td>\n",
       "      <td>辟谣 里昂 官方 否认 费 基尔 加盟 利物浦 难道 是 价格 没 谈拢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321190</th>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "      <td>萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思</td>\n",
       "      <td>10 大 最 让 美国 人 相信 的 荒诞 谣言 如 蜥蜴人 掌控 着 美国</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321189</th>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>萨达姆 此项 计划 没有 此国 破坏 的话 美国 还 会 对 伊拉克 发动战争 吗</td>\n",
       "      <td>萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title1_zh                    title2_zh  \\\n",
       "id                                                                     \n",
       "321187  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？   \n",
       "321190              萨达姆被捕后告诫美国的一句话，发人深思    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国   \n",
       "321189    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗          萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "\n",
       "                                    title1_tokenized  \\\n",
       "id                                                     \n",
       "321187  萨拉 赫 人气 爆棚 埃及 总统大选 未 参选 获 百万 选票 现任 总统 压力 山 大   \n",
       "321190                    萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思   \n",
       "321189     萨达姆 此项 计划 没有 此国 破坏 的话 美国 还 会 对 伊拉克 发动战争 吗   \n",
       "\n",
       "                              title2_tokenized  \n",
       "id                                              \n",
       "321187    辟谣 里昂 官方 否认 费 基尔 加盟 利物浦 难道 是 价格 没 谈拢  \n",
       "321190  10 大 最 让 美国 人 相信 的 荒诞 谣言 如 蜥蜴人 掌控 着 美国  \n",
       "321189              萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.fillna('UNKNOWN', inplace=True)\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c1e80b6-2f75-46bc-8f95-47e1eba04855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9999952e-01, 1.9164672e-19, 4.2101283e-07],\n",
       "       [1.0000000e+00, 5.6914282e-13, 9.1929320e-12],\n",
       "       [9.8432922e-01, 1.5668621e-02, 2.2136023e-06],\n",
       "       [1.0000000e+00, 4.8881038e-13, 4.5146115e-10],\n",
       "       [9.9999809e-01, 1.8821100e-10, 1.8697018e-06]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_test = tokenizer.texts_to_sequences(test.title1_tokenized)\n",
    "x2_test = tokenizer.texts_to_sequences(test.title2_tokenized)\n",
    "\n",
    "x1_test = keras.preprocessing.sequence.pad_sequences(x1_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x2_test = keras.preprocessing.sequence.pad_sequences(x2_test, maxlen=MAX_SEQUENCE_LENGTH)    \n",
    "\n",
    "predictions = model.predict([x1_test, x2_test])\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a9dfe20-d556-4429-9a61-539fd195d959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321187</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321190</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321189</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>321193</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321191</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   Category\n",
       "0  321187  unrelated\n",
       "1  321190  unrelated\n",
       "2  321189  unrelated\n",
       "3  321193  unrelated\n",
       "4  321191  unrelated"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "test['Category'] = [index_to_label[idx] for idx in np.argmax(predictions, axis=1)]\n",
    "submission = test.loc[:, ['Category']].reset_index()\n",
    "submission.columns = ['Id', 'Category']\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4cb9b-90c7-4e50-a153-ce4eec8e14c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
