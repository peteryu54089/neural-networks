{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Learning Algorithm\n",
    "\n",
    "This is NOT covered in class, but it is in Hayken's book and Jim Keller's Introduction to Computational Intelligence book. \n",
    "\n",
    "I am adding this here to show you an early algorithm to learn the Perceptron (before backpropagation, evolutionary algorithms, etc.).\n",
    "\n",
    "Below, **the assumption is that our two classes are separable**. If not, then refer to the Pocket Algorithm (https://en.wikipedia.org/wiki/Perceptron or https://direct.mit.edu/books/book/1919/chapter-abstract/52703/Perceptron-Learning-and-the-Pocket-Algorithm?redirectedFrom=fulltext).\n",
    "\n",
    "First, how do we make a data set? (synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include our libs\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# lets make some normally distributed data\n",
    "mean = [0, 0] # let the mean (aka where that center of the distribution is at!) be located at (0,0)\n",
    "cov = [[1, 0], [0, 1]] # make the covariance matrix an identity matrix\n",
    "                       # covariance matrix: https://en.wikipedia.org/wiki/Covariance_matrix\n",
    "                       # identity matrix: https://en.wikipedia.org/wiki/Identity_matrix\n",
    "                       # this says, varies 1 in the x direction and 1 in the y direction (its circular)\n",
    "                       # and there is no \"rotation\" of our normal distribution \n",
    "# now, make that 2D (so (x,y)) data set of 100 points\n",
    "# note: the T is for transpose (https://en.wikipedia.org/wiki/Transpose)\n",
    "x, y = np.random.multivariate_normal(mean, cov, 100).T\n",
    "\n",
    "# plot it!\n",
    "plt.plot(x, y, 'x')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets make a two class data set, the above was just one Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 1\n",
    "c1_mean = [-2,-3] # notice that I moved the \"center\"\n",
    "c1_cov = [[1, 0], [0, 1]]\n",
    "c1_x = np.random.multivariate_normal(c1_mean, c1_cov, 100)\n",
    "\n",
    "# class 2\n",
    "c2_mean = [6,1] # same, different center\n",
    "c2_cov = [[4, 0], [0, 4]] # this time, its bigger because 4 variance vs 1\n",
    "c2_x = np.random.multivariate_normal(c2_mean, c2_cov, 100)\n",
    "\n",
    "# plot it\n",
    "plt.plot(c1_x[:,0], c1_x[:,1], 'bx')\n",
    "plt.plot(c2_x[:,0], c2_x[:,1], 'ro')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, back to the Perceptron \n",
    "\n",
    "Lets pick an initial (random) weight vector and see what we get ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "import math\n",
    "\n",
    "# make a random weight vector and bias\n",
    "w = ( np.random.rand(2) - 0.5 ) # np.random.rand is in [0,1], so this line makes \n",
    "w = w / LA.norm(w) # LA.norm gives the magnitude, so / mag gives us a unit length vector\n",
    "print(w)\n",
    "\n",
    "# plot the weight vector (remember, decision boundary is perpendicular to it)\n",
    "scaleweight = 3.0 # aka, controls how \"big\"/thick the lines are below that I draw\n",
    "plt.plot([0,scaleweight*w[0]], [0,scaleweight*w[1]], 'k')\n",
    "ww = np.zeros(2)\n",
    "Rad = math.pi / 180.0\n",
    "ww[0] = w[0] * math.cos(90.0 * Rad) - w[1] * math.sin(90.0 * Rad)\n",
    "ww[1] = w[0] * math.sin(90.0 * Rad) + w[1] * math.cos(90.0 * Rad)\n",
    "plt.plot([0,scaleweight*ww[0]], [0,scaleweight*ww[1]], 'g')\n",
    "\n",
    "# plot the data set\n",
    "plt.plot(c1_x[:,0], c1_x[:,1], 'bx')\n",
    "plt.plot(c2_x[:,0], c2_x[:,1], 'ro')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# make data set\n",
    "X = np.concatenate((c1_x, c2_x), axis=0) \n",
    "l1 = np.ones(c1_x.shape[0])\n",
    "l2 = np.zeros(c2_x.shape[0])\n",
    "L = np.concatenate((l1, l2), axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, odds are that did not automatically come up with a Perceptron that separates your two classes\n",
    "\n",
    "If it did, just re-run it, you want it to not work to start here ;-)\n",
    "\n",
    "Now, lets run a Perceptron learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "t = 0 # time step\n",
    "learn_rate = 2 # learning rate\n",
    "\n",
    "# put a max number of iterations\n",
    "for k in range(50):\n",
    "      \n",
    "    MistakeDecisions = np.zeros(X.shape[0])\n",
    "    DecisionSigns = np.zeros(X.shape[0])\n",
    "    \n",
    "    # fire each\n",
    "    for j in range(X.shape[0]):\n",
    "        v = np.dot(X[j,:],w)\n",
    "        if(v >= 0): \n",
    "            DecisionSigns[j] = 1\n",
    "        else:\n",
    "            DecisionSigns[j] = 0\n",
    "        if(L[j] != DecisionSigns[j]):\n",
    "            MistakeDecisions[j] = 1\n",
    "            \n",
    "    # time to quit?\n",
    "    if( np.sum(MistakeDecisions) == 0 ):\n",
    "        print('Stopped at iteration ' + str(k))\n",
    "        break\n",
    "   \n",
    "    # update \n",
    "    for j in range(X.shape[0]):\n",
    "        if( MistakeDecisions[j] == 1 ):\n",
    "            w = w + learn_rate * ((L[j] - DecisionSigns[j]) * X[j,:])\n",
    "    \n",
    "    t = t + 1\n",
    "    \n",
    "    # plot the decision boundary \n",
    "    ww = w / LA.norm(w)\n",
    "    \n",
    "    pl.clf()\n",
    "    scaleweight = 3.0\n",
    "    plt.plot([0,scaleweight*ww[0]], [0,scaleweight*ww[1]], 'k')\n",
    "\n",
    "    www = np.zeros(2)\n",
    "    Rad = math.pi / 180.0\n",
    "    www[0] = ww[0] * math.cos(90.0 * Rad) - ww[1] * math.sin(90.0 * Rad)\n",
    "    www[1] = ww[0] * math.sin(90.0 * Rad) + ww[1] * math.cos(90.0 * Rad)\n",
    "    plt.plot([0,scaleweight*www[0]], [0,scaleweight*www[1]], 'g')    \n",
    "    \n",
    "    # plot the data set\n",
    "    pl.plot(c1_x[:,0], c1_x[:,1], 'bx')\n",
    "    pl.plot(c2_x[:,0], c2_x[:,1], 'ro')\n",
    "    \n",
    "    # animation, so pause it!\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "    time.sleep(0.5)   \n",
    "    \n",
    "########################################################\n",
    "########################################################\n",
    "\n",
    "print('*****************************************')\n",
    "print('Final Plot')\n",
    "print('*****************************************')\n",
    "\n",
    "# plot the decision boundary \n",
    "ww = w / LA.norm(w)\n",
    "pl.clf()\n",
    "scaleweight = 3.0\n",
    "plt.plot([0,scaleweight*ww[0]], [0,scaleweight*ww[1]], 'k')\n",
    "\n",
    "www = np.zeros(2)\n",
    "Rad = math.pi / 180.0\n",
    "www[0] = ww[0] * math.cos(90.0 * Rad) - ww[1] * math.sin(90.0 * Rad)\n",
    "www[1] = ww[0] * math.sin(90.0 * Rad) + ww[1] * math.cos(90.0 * Rad)\n",
    "plt.plot([0,scaleweight*www[0]], [0,scaleweight*www[1]], 'g')    \n",
    "    \n",
    "# plot the data set\n",
    "pl.plot(c1_x[:,0], c1_x[:,1], 'bx')\n",
    "pl.plot(c2_x[:,0], c2_x[:,1], 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A   \n",
    "\n",
    "Address the following\n",
    "\n",
    "1) If the patterns overlap, what happens?\n",
    "\n",
    "2) Can you come up with another way to draw the decision surface?\n",
    "\n",
    "3) Can you figure out how to include a bias term?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
