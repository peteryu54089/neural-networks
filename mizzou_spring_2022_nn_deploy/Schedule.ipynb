{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | Topic | Help | Assignments |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| <b>Week1: January 18</b> | <br> Introduction to NN <br> Jupyter <br> Syllabus <br> McCulloch-Pitts <br> Perceptron <br> Geometric Interpretation <br> Digital Logic <br> Classification <br> Regression | [NN history](NN/NNHistory.ipynb) <br> [McCulloch-Pitts](NN/McCulloch-PittsNeuronInNumPy.ipynb) <br> [Perceptron in NumPy](NN/HandCodedPerceptronInNumPy.ipynb) <br> [Perceptron Learning Algorithm](NN/PerceptronLearningAlgorithm_InNumPy.ipynb) <br> <a href=\"http://hagan.okstate.edu/4_Perceptron.pdf\">Perceptron Learning Alg</a> <br> [Perceptron in PyTorch](NN/PerceptronPyTorch.ipynb) <br> Symbolic Manip [link1](https://www.symbolab.com) [link2](https://www.wolframalpha.com) <br> <a href=\"https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/linear_regression/main.py#L22-L23\">Linear Regression PyTorch</a> | - |\n",
    "| <b>Week1: January 20</b> | Multi-Layer Perceptron (MLP) <br> One Hot Encoding <br> Backpropagation | [Play with MLP](NN/SeeMlpInAction.ipynb) <br> [TensorBoard](NN/TensorBoard.ipynb) <br> [TensorBoardDataLoader](NN/TensorBoard2.ipynb) <br> [MLP Online](NN/SeeMlpInAction.ipynb) <br> [BackProp](NN/BackProp.ipynb) <br> [BackProp NumPy](NN/BackPropInNumPy.ipynb) <br> [MLP PyTorch](NN/MlpInPyTorch.ipynb) <br> [MiniBatch](NN/MiniBatch.ipynb) <br> <a href=\"https://github.com/yunjey/pytorch-tutorial\">Nice PyTorch Example</a> | - |\n",
    "| |\n",
    "| <b>Week2: January 25</b> | MLP Day II | - | - |\n",
    "| <b>Week2: January 27</b> | Radial Basis Functions/Nets (RBF/RBN) | - | Project 1 Assign <br> (Supervised Learning) |\n",
    "| |\n",
    "| <b>Week3: February 1</b> | Shared Weight NNs (SWNNs) <br> Aggregation Pooling <br> What is Convolution? | Chain Rule and Total Deriv <a href=\"http://sites.science.oregonstate.edu/math/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/chain/chain.html\">1</a>, <a href=\"https://tutorial.math.lamar.edu/classes/calciii/chainrule.aspx\">2</a>,  <a href=\"https://en.wikipedia.org/wiki/Total_derivative\">3</a> | - |\n",
    "| <b>Week3: February 3</b> | Loss Functions <br> Convolution Neural Nets (CNN) | [Simple PyTorch CNN](NN/SimpleCNN.ipynb) <br> <a href=\"https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main.py#L35-L56\">Different Clean CNN Example</a> <br> <a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html\">Activation Functions</a> | - |\n",
    "| |\n",
    "| <b>Week4: February 8</b> | Mini-Batch <br> Fast Conv <br> Evaluation Metrics <br> Calculus on Computational Graphs <br> Autograd | [Optimization Part 1](NN/Opt1.ipynb) <br> [AutoGrad](NN/AutoGrad.ipynb) <br> [Confusion Matrix Code](NN/ConfMat.ipynb) <br> <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">Confusion Matrices</a> <br> <a href=\"https://towardsdatascience.com/how-are-convolutions-actually-performed-under-the-hood-226523ce7fbf\">Conv Under the Hood</a> | - |\n",
    "| <b>Week4: February 10</b> | More Advanced Optimization - Part I <br> *Newton, Nestrov, Adagrad* | <a href=\"http://www.seas.ucla.edu/~vandenbe/236C/lectures/qnewton.pdf\">Broyden–Fletcher–Goldfarb–Shanno</a> <br> <a href=\"https://aria42.com/blog/2014/12/understanding-lbfgs\">Another BFGS link</a> <br> <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.BFGS.html\">Python BFGS</a> <br> <a href=\"https://arxiv.org/pdf/1802.05374.pdf\">BFGS in deep learning</a> | - |\n",
    "| |\n",
    "| <b>Week5: February 15</b> | More Advanced Optimization - Part II <br> *RMSprop and ADAM* | [Optimization Part 2](NN/Opt2.ipynb) | [UFA](NN/UFA.ipynb) |\n",
    "| <b>Week5: February 17</b> | Regularization <br> Drop Out | [Dropout Code](NN/Drop.ipynb) <br> <a href=\"https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\">Dropout Paper</a> <br> [Reg Code](NN/Reg.ipynb) | - |\n",
    "| |\n",
    "| <b>Week6: February 22</b> | Autoencoders | [Simple AE](NN/Autoencoder.ipynb) <br> <a href=\"https://arxiv.org/abs/1804.10253\">AE and PCA</a> | <font color=\"red\">Exam 1</font> |\n",
    "| <b>Week6: February 24</b> | Generative Autoencoders | - | - |\n",
    "| |\n",
    "| <b>Week7: March 1</b> | Hebbian learning <br> Self Organizing Maps and Neual Gas | [Simple SOM](NN/SOM.ipynb) <br> <a href=\"https://github.com/itdxer/neupy/blob/master/notebooks/Looking%20inside%20of%20the%20VGG19%20using%20SOFM.ipynb\">SOM on VGG19</a> <br> <a href=\"https://github.com/itdxer/neupy/blob/master/examples/competitive/sofm_digits.py\">SOM on MNIST</a> | <font color=\"purple\">Project 1 Due</font> <br> Project 2 Assign <br> (Unsupervised or Self-Supervised Learning) |\n",
    "| <b>Week8: March 3</b> | Growing Neural Gas | <a href=\"https://github.com/itdxer/neupy/blob/master/notebooks/growing-neural-gas/Making%20Art%20with%20Growing%20Neural%20Gas.ipynb\">GNG Code</a> <br> <a href=\"http://neupy.com/2018/03/26/making_art_with_growing_neural_gas.html\">GNG Animation</a> <br> <a href=\"http://ftp.ks.uiuc.edu/Publications/Papers/PDF/MART91B/MART91B.pdf\">NG Paper</a> <br> <a href=\"https://papers.nips.cc/paper/893-a-growing-neural-gas-network-learns-topologies.pdf\">GNG Paper</a> | - |\n",
    "| |\n",
    "| <b>Week9: March 8</b> | Self-Supervised Learning | - | - |\n",
    "| <b>Week14: April 10</b> | Adaptive neural fuzzy inference (ANFIS) | <a href=\"https://github.com/Blake-Ruprecht/Fuzzy-Fusion\">PyTorch Implementation</a> <br> <a href=\"https://www.researchgate.net/publication/3113825_ANFIS_Adaptive-Network-based_Fuzzy_Inference_System\">ANFIS Paper</a> | - |\n",
    "| |\n",
    "| <b>Week9: March 15</b> | Recurrent Neural Networks (RNN), BTT, and TBTT | <a href=\"https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html\">RNN from Scratch in PyTorch</a> <br> <a href=\"https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/recurrent_neural_network/main.py#L39-L58\">PyTorch Example</a> <br> <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\">RNN Code</a> <br> <a href=\"https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\">Tutorial</a> <br> <a href=\"http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf\">BTT and TBTT</a> <br> <a href=\"https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html?highlight=rnn\">Example 1</a> <br> <a href=\"https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html?highlight=rnn\">Example 2</a> <br> <a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\">RNN Walk Through</a> | - |\n",
    "| <b>Week10: March 17</b> | Long Short Term Memory (LSTM) | <a href=\"https://www.bioinf.jku.at/publications/older/2604.pdf\">LSTM Paper</a> <br> <a href=\"https://cnl.salk.edu/~schraudo/pubs/GerSchSch02.pdf\">Peepholes</a> <br> <a href=\"https://arxiv.org/pdf/1503.04069.pdf\">LSTM Comparisons</a> <br> <a href=\"https://arxiv.org/abs/1406.1078\">Gated recurrent units</a> <br> <a href=\"https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\">Cute Animations</a> <br> <a href=\"https://blog.floydhub.com/gru-with-pytorch/\">GRU PyTorch Link</a> <br> <a href=\"https://arxiv.org/pdf/1803.06396.pdf\">BPTT Variants</a> <br> <a href=\"https://arxiv.org/pdf/1810.04020.pdf\">Image Captioning Survey</a> <br> <a href=\"https://arxiv.org/pdf/1610.02583.pdf\">BPTT Walk Through</a> | <font color=\"green\">Project 3 Assign</font> <br> (Temporal/Sequence) |\n",
    "| |\n",
    "| <b>Week10: March 22</b> | LSTM Part II | <a href=\"https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\">Fun Animated Example</a> <br> <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">Python LSTM Reference</a> <br> <a href=\"https://statisticalinterference.wordpress.com/2017/06/01/lstms-in-even-more-excruciating-detail/\">Numeric Example</a> | - |\n",
    "| <b>Week12: March 24</b> | TBD | - | - |\n",
    "| |\n",
    "| <b>Week11: March 29</b> | SPRING BREAK | - | - |\n",
    "| <b>Week11: March 31</b> | SPRING BREAK | - | - |\n",
    "| |\n",
    "| <b>Week12: April 5</b> | Recurrent Networks | [RNet](NN/RNN.ipynb) | <font color=\"purple\">Project 2 Due</font> | - |\n",
    "| <b>Week12: April 7</b> | Aggregation Networks | <a href=\"https://ieeexplore.ieee.org/document/159064\">Logic Example</a> |\n",
    "| |\n",
    "| <b>Week13: April 12</b> | Data fusion networks | <a href=\"https://arxiv.org/abs/1905.04394\">Fuzzy integral</a> <br> <a href=\"https://github.com/aminb99/choquet-integral-NN\">Example Code</a> <br> <a href=\"http://derektanderson.com/pdfs/LOSN.pdf\">LOSN</a> <br> <a href=\"https://github.com/charlieveal/LOSN\">Code</a> <br> <a href=\"https://spie.org/Publications/Proceedings/Paper/10.1117/12.544320?SSO=1\">FOWA</a> <br> <a href=\"https://ieeexplore.ieee.org/document/1375789\">FOWA 2</a> <br> <a href=\"https://ieeexplore.ieee.org/document/4069108\">ROCA</a> | - |\n",
    "| <b>Week13: April 14</b> | Siamese Networks and Metric Learning | <a href=\"https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\">Siamese Net 1</a> <br> <a href=\"https://www.coursera.org/lecture/convolutional-neural-networks/siamese-network-bjhmj\">Siamese Net 2</a> <br> <a href=\"https://openaccess.thecvf.com/content_ICCV_2019/papers/Roy_Siamese_Networks_The_Tale_of_Two_Manifolds_ICCV_2019_paper.pdf\">Siamese Net 3</a> <br> <a href=\"https://gombru.github.io/2019/04/03/ranking_loss\">Contrastive Loss 1</a> <br> <a href=\"https://ml-compiled.readthedocs.io/en/latest/loss_functions.html\">Contrastive Loss 2</a> <br> <a href=\"https://medium.com/@enoshshr/triplet-loss-and-siamese-neural-networks-5d363fdeba9b\">Triplet Loss 1</a> <br> <a href=\"https://en.wikipedia.org/wiki/Triplet_loss\">Triplet Loss 2</a> <br> <a href=\"https://www.jmlr.org/papers/volume11/chechik10a/chechik10a.pdf\">Triplet Loss 3</a> <br> <a href=\"https://arxiv.org/pdf/1503.03832.pdf\">Triplet Loss 4</a> <br> <a href=\"https://www.coursera.org/lecture/convolutional-neural-networks/triplet-loss-HuUtN\">Triplet Loss 5</a> <br> <a href=\"http://people.bu.edu/bkulis/pubs/ftml_metric_learning.pdf\">Metric Learning</a> | - |\n",
    "| |\n",
    "| <b>Week14: April 19</b> | Mathematical Morphology Neural Nets and Shape | [PyTorch Code](NN/Morph.ipynb) <br> <a href=\"https://arxiv.org/abs/1912.02259\">Deep MSNN</a> <br> <a href=\"https://arxiv.org/abs/1811.12231\">Shape</a> <br> <a href=\"https://arxiv.org/pdf/1906.01751.pdf\">Deep morph</a> | - |\n",
    "| <b>Week14: April 21</b> | \"Deconvolution\" | [Fractional Strided Convolution](NN/Deconv.ipynb)<br> [Grad-CAM](NN/gradcam.ipynb) | - |\n",
    "| |\n",
    "| <b>Week15: April 26</b> | Deep Reinforcement Learning Part I | - | <font color=\"purple\">Project 3 Due</font> |\n",
    "| <b>Week15: April 28</b> | Deep Reinforcement Learning Part II | - | - |\n",
    "| |\n",
    "| <b>Week16: May 3</b> | Transformers Part I | - | - |\n",
    "| <b>Week16: May 5</b> | Transformers Part II | - | <font color=\"red\">Exam 2</font> |\n",
    "| |\n",
    "| <b>FINAL</b> | Final Project | - | <font color=\"purple\">Presentations</font> |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
